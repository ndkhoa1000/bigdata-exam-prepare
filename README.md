# bigdata-exam-prepare

---

**Preconfigured Hadoop instance (zip)** ‚Äî Google Drive:
[https://drive.google.com/file/d/1gdxQmwJujaHysIUgGj8pEObecWYbIY_i/view?usp=sharing](https://drive.google.com/file/d/1gdxQmwJujaHysIUgGj8pEObecWYbIY_i/view?usp=sharing)

**Install manual (Firefox or via `gdown`)**
(install `gdown` with `pip` if needed)

```bash
gdown https://drive.google.com/uc?id=1gdxQmwJujaHysIUgGj8pEObecWYbIY_i
```

---

Ghi ch√∫ c√†i Hadoop th·ªß c√¥ng, file m·∫´u MapReduce (d√πng Maven) v√† Makefile ƒë·ªÉ ƒë∆°n gi·∫£n ho√° vi·ªác test:
[https://github.com/ndkhoa1000/hadoop-cluster-installation](https://github.com/ndkhoa1000/hadoop-cluster-installation)

---

# üêò HADOOP MAPREDUCE CHEAT SHEET

**(Cheatsheet cho MapReduce ‚Äî t·ªïng h·ª£p c√°c m·∫´u thi·∫øt k·∫ø ph·ªï bi·∫øn + test m·∫´u)**

## üìö M·ª•c l·ª•c

1. [Filtering Pattern (B·ªô l·ªçc)](#filtering)
2. [Numerical Aggregation (Th·ªëng k√™ s·ªë li·ªáu)](#aggregation)
3. [Inverted Index (Ch·ªâ m·ª•c ng∆∞·ª£c)](#inverted)
4. [Distinct Pattern (Lo·∫°i b·ªè tr√πng l·∫∑p)](#distinct)
5. [Top-K Pattern (T√¨m Top N)](#topk)
6. [Driver Configuration (C·∫•u h√¨nh Job)](#driver)

---

<a name="filtering"></a>

# 1. üîç FILTERING PATTERN (B·ªô L·ªçc)

**M·ª•c ƒë√≠ch:** T∆∞∆°ng t·ª± m·ªánh ƒë·ªÅ `WHERE` trong SQL
**ƒê·∫∑c ƒëi·ªÉm:** Ch·ªâ c·∫ßn Mapper ‚Üí `job.setNumReduceTasks(0)`

### ‚úÖ V√≠ d·ª• 1 (D·ªÖ): L·ªçc theo t·ª´ kh√≥a (Log Analysis) ‚Äî t√¨m d√≤ng ch·ª©a `"ERROR"`

**Input**

```
INFO: Server started successfully.
ERROR: Database connection failed.
WARN: High memory usage.
ERROR: NullPointerException at line 42.
```

**Output mong ƒë·ª£i**

```
ERROR: Database connection failed.
ERROR: NullPointerException at line 42.
```

**Mapper**

```java
public void map(Object key, Text value, Context context) {
    String line = value.toString();
    if (line.contains("ERROR")) {
        context.write(value, NullWritable.get());
    }
}
```

---

### ‚úÖ V√≠ d·ª• 2 (Trung b√¨nh): L·ªçc theo ƒëi·ªÅu ki·ªán s·ªë h·ªçc ‚Äî giao d·ªãch > 1000$

**Input**

```
TX01,UserA,500.0
TX02,UserB,1500.0
TX03,UserC,200.0
TX04,UserD,5000.0
```

**Output mong ƒë·ª£i**

```
TX02,UserB,1500.0
TX04,UserD,5000.0
```

**Mapper**

```java
public void map(Object key, Text value, Context context) {
    String[] parts = value.toString().split(",");
    if (parts.length >= 3) {
        try {
            double amount = Double.parseDouble(parts[2]);
            if (amount > 1000.0) {
                context.write(value, NullWritable.get());
            }
        } catch (NumberFormatException e) {}
    }
}
```

---

### ‚úÖ V√≠ d·ª• 3 (Trung b√¨nh): L·ªçc theo kho·∫£ng th·ªùi gian ‚Äî ch·ªâ d·ªØ li·ªáu nƒÉm 2024

**Input**

```
2023-12-31, Sales: 100
2024-01-01, Sales: 200
2024-05-20, Sales: 150
2022-10-10, Sales: 90
```

**Output mong ƒë·ª£i**

```
2024-01-01, Sales: 200
2024-05-20, Sales: 150
```

**Mapper**

```java
public void map(Object key, Text value, Context context) {
    String line = value.toString();
    if (line.startsWith("2024")) {
        context.write(value, NullWritable.get());
    }
}
```

---

<a name="aggregation"></a>

# 2. üìä NUMERICAL AGGREGATION (Th·ªëng K√™ S·ªë Li·ªáu)

**M·ª•c ƒë√≠ch:** GROUP BY, SUM, COUNT, AVG

### ‚úÖ V√≠ d·ª• 1 (D·ªÖ): Word Count

**Input**

```
Hello Hadoop
Hello World
```

**Output**

```
Hadoop  1
Hello   2
World   1
```

**Mapper**

```java
StringTokenizer itr = new StringTokenizer(value.toString());
while (itr.hasMoreTokens()) {
    word.set(itr.nextToken());
    context.write(word, new IntWritable(1));
}
```

**Reducer**

```java
int sum = 0;
for (IntWritable val : values) sum += val.get();
context.write(key, new IntWritable(sum));
```

---

### ‚úÖ V√≠ d·ª• 2 (Trung b√¨nh): T√≠nh T·ªïng Doanh Thu theo Store

**Input**

```
StoreA,100
StoreB,200
StoreA,300
```

**Output**

```
StoreA  400
StoreB  200
```

**Mapper**

```java
String[] parts = value.toString().split(",");
context.write(new Text(parts[0]), new IntWritable(Integer.parseInt(parts[1])));
```

**Reducer**

```java
int total = 0;
for (IntWritable val : values) total += val.get();
context.write(key, new IntWritable(total));
```

---

### ‚úÖ V√≠ d·ª• 3 (Kh√≥ h∆°n): T√≠nh ƒëi·ªÉm trung b√¨nh (Average)

**Input**

```
Math,8.0
Math,10.0
Physics,7.0
```

**Output**

```
Math    9.0
Physics 7.0
```

**Reducer**

```java
double sum = 0;
int count = 0;
for (DoubleWritable val : values) {
    sum += val.get();
    count++;
}
context.write(key, new DoubleWritable(sum / count));
```

---

<a name="inverted"></a>

# 3. üìñ INVERTED INDEX (Ch·ªâ M·ª•c Ng∆∞·ª£c)

**M·ª•c ƒë√≠ch:** X√¢y d·ª±ng ch·ªâ m·ª•c cho t√¨m ki·∫øm (term ‚Üí file, v·ªã tr√≠)

### ‚úÖ V√≠ d·ª• 1 (D·ªÖ): T·ª´ kh√≥a ‚Üí T√™n file

**Input**

```
doc1.txt: "apple banana"
doc2.txt: "banana cherry"
```

**Output**

```
apple   doc1.txt
banana  doc1.txt, doc2.txt
cherry  doc2.txt
```

**Mapper**

```java
FileSplit fileSplit = (FileSplit) context.getInputSplit();
String fileName = fileSplit.getPath().getName();
context.write(new Text(word), new Text(fileName));
```

**Reducer**

```java
StringBuilder sb = new StringBuilder();
for (Text val : values) sb.append(val.toString()).append(", ");
context.write(key, new Text(sb.toString()));
```

---

### ‚úÖ V√≠ d·ª• 2 (Trung b√¨nh): T·ª´ kh√≥a ‚Üí File:LineNumber

**Input**

```
log.txt (line 10): "Error: Fail"
log.txt (line 50): "Error: Timeout"
```

**Output**

```
Error   log.txt:Line10, log.txt:Line50
```

**Mapper**

```java
long lineNum = ((LongWritable) key).get();
context.write(new Text(word), new Text(fileName + ":Line" + lineNum));
```

**Reducer:** n·ªëi string t∆∞∆°ng t·ª± v√≠ d·ª• tr√™n

---

<a name="distinct"></a>

# 4. üßπ DISTINCT PATTERN (Lo·∫°i B·ªè Tr√πng L·∫∑p)

**M·ª•c ƒë√≠ch:** SELECT DISTINCT / Dedup

### ‚úÖ V√≠ d·ª• 1 (D·ªÖ): Danh s√°ch User ID duy nh·∫•t

**Input**

```
user1
user2
user1
user3
user2
```

**Output**

```
user1
user2
user3
```

**Mapper & Reducer**

```java
context.write(key, NullWritable.get());
```

---

### ‚úÖ V√≠ d·ª• 2 (Trung b√¨nh): Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p (Dedup)

**Input**

```
A, 10
B, 20
A, 10
```

**Output**

```
A, 10
B, 20
```

**Mapper**

```java
context.write(value, NullWritable.get());
```

---

<a name="topk"></a>

# 5. üèÜ TOP K PATTERN (T√¨m Top N)

**M·ª•c ƒë√≠ch:** T√¨m c√°c ph·∫ßn t·ª≠ n·ªïi b·∫≠t nh·∫•t (Top-K)

### ‚úÖ V√≠ d·ª• 1 (D·ªÖ): Top 2 t·ª´ d√†i nh·∫•t

**Input**

```
ant
hippopotamus
elephant
cat
```

**Output mong ƒë·ª£i**

```
hippopotamus
elephant
```

**Mapper (gi·ªØ c·∫•u tr√∫c TreeMap ƒë·ªÉ track Top N)**

```java
topWords.put(word.length(), new Text(word));
if (topWords.size() > 2) topWords.remove(topWords.firstKey()); // Gi·ªØ Top 2
```

---

### ‚úÖ V√≠ d·ª• 2 (Trung b√¨nh): Top s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t

**Input**

```
Phone, 500
Laptop, 1000
Mouse, 20
```

**Output mong ƒë·ª£i**

```
Laptop (1000)
```

**Mapper**

```java
topProducts.put(sales, new Text(product));
if (topProducts.size() > 1) topProducts.remove(topProducts.firstKey());
```

---

<a name="driver"></a>

# ‚öôÔ∏è DRIVER CONFIGURATION (C·∫•u h√¨nh Job)

* Set Output Value l√† s·ªë nguy√™n:

```java
job.setOutputValueClass(IntWritable.class);
```

* Set Output Value l√† s·ªë th·ª±c:

```java
job.setOutputValueClass(DoubleWritable.class);
```

* Map ra Int, nh∆∞ng Reduce ra Text (v√≠ d·ª• MapOutput kh√°c v·ªõi Output):

```java
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(IntWritable.class);
```

* T·ªëi ∆∞u h√≥a (Combiner):

```java
job.setCombinerClass(IntSumReducer.class);
```

* Job ch·ªâ ch·∫°y Mapper (L·ªçc):

```java
job.setNumReduceTasks(0);
```

---

## üîß Th√™m t√†i nguy√™n & c√¥ng c·ª• (g·ª£i √Ω)

* Google Drive preconfig zip (d√πng `gdown` ƒë·ªÉ t·∫£i): [https://drive.google.com/file/d/1gdxQmwJujaHysIUgGj8pEObecWYbIY_i/view?usp=sharing](https://drive.google.com/file/d/1gdxQmwJujaHysIUgGj8pEObecWYbIY_i/view?usp=sharing)
* H∆∞·ªõng d·∫´n c√†i + repo m·∫´u (Maven + Makefile): [https://github.com/ndkhoa1000/hadoop-cluster-installation](https://github.com/ndkhoa1000/hadoop-cluster-installation)
